{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b6937e4",
   "metadata": {},
   "source": [
    "# Convert the High Level Features Dataset from Parquet to TFRecord\n",
    "This converts the High Level feature classifier training and test datasets to TFRecord format\n",
    " - Reads Parquet data into a dataframe\n",
    " - Save the dataframe as TFRecords using spark-tensorflow-connector\n",
    "   (see https://github.com/tensorflow/ecosystem/tree/master/spark/spark-tensorflow-connector)\n",
    "\n",
    "Data in TFRecord format can be processed natively using TensorFlow and the tf.data API. Other formats, notably Apache Parquet, cannot be fed directly into TensorFlow, but require adapters, for example the Petastorm library.  \n",
    "The TFRecord dataset is used as input for the TensorFlow Keras with TFRecord example notebook. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79c44b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/08 16:27:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# No need to run this when using CERN SWAN service\n",
    "# Just add the configuration parameters for Spark on the \"star\" button integration\n",
    "\n",
    "# pip install pyspark or use your favorite way to set Spark Home, here we use findspark\n",
    "import findspark\n",
    "findspark.init('/home/luca/Spark/spark-3.3.2-bin-hadoop3') #set path to SPARK_HOME\n",
    "\n",
    "# Create Spark session and configure according to your environment\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark-Tensorflow connector for scala 2.12\n",
    "# for spark 2.4.8 and scala 2.11 use from maven central: \n",
    "# --packages org.tensorflow:spark-tensorflow-connector_2.11:1.14.0\n",
    "JAR = \"http://canali.web.cern.ch/res/spark-tensorflow-connector_2.12-1.11.0.jar\"\n",
    "\n",
    "spark = ( SparkSession.builder\n",
    "            .appName(\"Prepare TFRecord dataset\")\n",
    "            .master(\"local[*]\")\n",
    "            .config(\"spark.driver.memory\",\"2g\")\n",
    "            .config(\"spark.jars\", JAR)\n",
    "            .config(\"spark.ui.showConsoleProgress\", \"false\")\n",
    "            .getOrCreate()\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7752316",
   "metadata": {},
   "source": [
    "### Define the input datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87bcf4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- HLF_input: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- encoded_label: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      "\n",
      "root\n",
      " |-- HLF_input: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- encoded_label: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the datasets from https://github.com/cerndb/SparkDLTrigger/tree/master/Data\n",
    "#\n",
    "# For CERN users, data is available on EOS\n",
    "PATH = \"/eos/project/s/sparkdltrigger/public/\"\n",
    "\n",
    "# PATH = \"../Data/sparkdltrigger.web.cern.ch/sparkdltrigger/\"\n",
    "\n",
    "df_test = spark.read.parquet(PATH + \"testUndersampled_HLF_features.parquet\")\n",
    "df_test.printSchema()\n",
    "\n",
    "df_train = spark.read.parquet(PATH + \"trainUndersampled_HLF_features.parquet\")\n",
    "df_train.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fcd5dc",
   "metadata": {},
   "source": [
    "### Save the datasets in TFRecord format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e481f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize the output path\n",
    "outputPATH = PATH\n",
    "\n",
    "# Process the test dataset\n",
    "# compact output in 2 files with coalesce(2)\n",
    "df_test.coalesce(2).write.format(\"tfrecords\").save(outputPATH + \"testUndersampled_HLF_features.tfrecord\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e82f80ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for the training dataset\n",
    "\n",
    "# save the training dataset in TFRecord format\n",
    "# compact output in 4 files with coalesce(4)\n",
    "df_train.coalesce(4).write.format(\"tfrecords\").save(outputPATH + \"trainUndersampled_HLF_features.tfrecord\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f211a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e4f01b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
