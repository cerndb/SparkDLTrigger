{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b6937e4",
   "metadata": {},
   "source": [
    "# Extract the High Level Features Dataset from the full dataset\n",
    "This extracts the High Level features training and test datasets from the full datasets.  \n",
    "It is used as input for the TensorFlow and Petastorm example notebooks.  \n",
    "Key actions are:\n",
    " - Converting Vectors into Arrays\n",
    " - Saving the resulting Spark DataFrames as Parquet files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79c44b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/03/08 16:44:13 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "23/03/08 16:44:33 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!\n"
     ]
    }
   ],
   "source": [
    "# No need to run this when using CERN SWAN service\n",
    "# Just add the configuration parameters for Spark on the \"star\" button integration\n",
    "\n",
    "# pip install pyspark or use your favorite way to set Spark Home, here we use findspark\n",
    "import findspark\n",
    "findspark.init('/home/luca/Spark/spark-3.3.2-bin-hadoop3') #set path to SPARK_HOME\n",
    "\n",
    "# Create Spark session and configure according to your environment\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = ( SparkSession.builder\n",
    "            .appName(\"Prepare TFRecord dataset\")\n",
    "            .master(\"yarn\")\n",
    "            .config(\"spark.driver.memory\",\"2g\")\n",
    "            .config(\"spark.executor.memory\",\"32g\")\n",
    "            .config(\"spark.executor.cores.memory\",\"6\")       \n",
    "            .config(\"spark.ui.showConsoleProgress\", \"false\")\n",
    "            .getOrCreate()\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6a08b2",
   "metadata": {},
   "source": [
    "### Define and prepare the input datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87bcf4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hfeatures: vector (nullable = true)\n",
      " |-- label: long (nullable = true)\n",
      " |-- lfeatures: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |-- hfeatures_dense: vector (nullable = true)\n",
      " |-- encoded_label: vector (nullable = true)\n",
      " |-- HLF_input: vector (nullable = true)\n",
      " |-- GRU_input: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      "\n",
      "root\n",
      " |-- hfeatures: vector (nullable = true)\n",
      " |-- label: long (nullable = true)\n",
      " |-- lfeatures: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |-- hfeatures_dense: vector (nullable = true)\n",
      " |-- encoded_label: vector (nullable = true)\n",
      " |-- HLF_input: vector (nullable = true)\n",
      " |-- GRU_input: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the datasets from https://github.com/cerndb/SparkDLTrigger/tree/master/Data\n",
    "#\n",
    "# For CERN users, data is available on EOS and HDFS\n",
    "# PATH = \"/eos/project/s/sparkdltrigger/public/\"\n",
    "PATH = \"hdfs://analytix/Training/Spark/TopologyClassifier/\"\n",
    "\n",
    "df_test_raw = spark.read.parquet(PATH + \"testUndersampled.parquet\")\n",
    "df_test_raw.printSchema()\n",
    "\n",
    "df_train_raw = spark.read.parquet(PATH + \"trainUndersampled.parquet\")\n",
    "df_train_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07441fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- HLF_input: array (nullable = false)\n",
      " |    |-- element: double (containsNull = false)\n",
      " |-- encoded_label: array (nullable = false)\n",
      " |    |-- element: double (containsNull = false)\n",
      "\n",
      "root\n",
      " |-- HLF_input: array (nullable = false)\n",
      " |    |-- element: double (containsNull = false)\n",
      " |-- encoded_label: array (nullable = false)\n",
      " |    |-- element: double (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extacts the columns of interest for the high level features classifier\n",
    "# Transform Vectors in Arrays\n",
    "# This is because we need Array for the Petastorm example notebook\n",
    "\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "df_test = ( df_test_raw\n",
    "             .withColumn('HLF_input', vector_to_array('HLF_input'))\n",
    "             .withColumn('encoded_label', vector_to_array('encoded_label'))\n",
    "             .select('HLF_input', 'encoded_label')\n",
    "          )\n",
    "\n",
    "df_test.printSchema()\n",
    "\n",
    "df_train = ( df_train_raw\n",
    "               .withColumn('HLF_input', vector_to_array('HLF_input'))\n",
    "               .withColumn('encoded_label', vector_to_array('encoded_label'))\n",
    "               .select('HLF_input', 'encoded_label')\n",
    "           )\n",
    "\n",
    "df_train.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c529482c",
   "metadata": {},
   "source": [
    "### Save the datasets in Parquet format\n",
    "\n",
    "An additional (and optional) tuning is to set the Parquet block size of 1MB, this forces row groups to 1MB. \n",
    "This action is motivated by the use of Petastorm.  \n",
    "Petastorm by default uses Parquet block size in make_batch_reader to determine the batch size to feed to Tensorflow.  \n",
    "Note it is also possible to change the batch size in Petastorm, so this is not strictly necessary + using a rowgroup size of 1MB is not ideal in many cases (too small as a rowgroup and too large as a batch size).  \n",
    "If you don't need to use Petastorm, you can skip the setting option(\"parquet.block.size\", 1024 * 1024) and use defaults (128 MB for the rowgropup size).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e481f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize the output path\n",
    "outputPATH = PATH\n",
    "\n",
    "# Process the test dataset\n",
    "# compact output in 1 file with coalesce(1)\n",
    "# this will force the operation to run on 1 task only, \n",
    "# so it's a bit slow but we get the output in compact form\n",
    "\n",
    "( df_test \n",
    "    .coalesce(1)\n",
    "    .write\n",
    "    .option(\"parquet.block.size\", 1024 * 1024)\n",
    "    .parquet(outputPATH + \"testUndersampled_HLF_features.parquet\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e82f80ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for the training dataset\n",
    "\n",
    "# compact output in 4 files with coalesce(4)\n",
    "( df_train \n",
    "    .coalesce(4)\n",
    "    .write\n",
    "    .option(\"parquet.block.size\", 1024 * 1024)\n",
    "    .parquet(outputPATH + \"trainUndersampled_HLF_features.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b75fc7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1ec9ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
