{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b6937e4",
   "metadata": {},
   "source": [
    "# Prepare the Inclusive Classifier Dataset using TFRecord and Parquet\n",
    "This converts the full training and test datasets into TFRecord format and Parquet format for tensorFlow and for Petasorm\n",
    " - Maps Parquet data into a dataframe\n",
    " - Save the dataframe as TFRecords using spark-tensorflow-connector\n",
    "   (see https://github.com/tensorflow/ecosystem/tree/master/spark/spark-tensorflow-connector)\n",
    "\n",
    "Data in TFRecord format can be processed natively using TensorFlow and the tf.data API. Other formats, notably Apache Parquet, cannot be fed directly into TensorFlow, but require adapters, for example the Petastorm library.  \n",
    "The TFRecord dataset is used as input for the TensorFlow Keras with TFRecord example notebook. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79c44b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to run this when using CERN SWAN service\n",
    "# Just add the configuration parameters for Spark on the \"star\" button integration\n",
    "\n",
    "# pip install pyspark or use your favorite way to set Spark Home, here we use findspark\n",
    "import findspark\n",
    "findspark.init('/home/luca/Spark/spark-3.3.2-bin-hadoop3') #set path to SPARK_HOME\n",
    "\n",
    "# Create Spark session and configure according to your environment\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark-Tensorflow connector for scala 2.12\n",
    "# for spark 2.4.8 and scala 2.11 use from maven central: \n",
    "# --packages org.tensorflow:spark-tensorflow-connector_2.11:1.14.0\n",
    "JAR = \"http://canali.web.cern.ch/res/spark-tensorflow-connector_2.12-1.11.0.jar\"\n",
    "\n",
    "spark = ( SparkSession.builder\n",
    "            .appName(\"Prepare TFRecord dataset\")\n",
    "            .master(\"yarn\")\n",
    "            .config(\"spark.driver.memory\",\"2g\")\n",
    "            .config(\"spark.jars\", JAR)\n",
    "            .config(\"spark.executor.memory\",\"64g\")\n",
    "            .config(\"spark.executor.cores.memory\",\"8\")   \n",
    "            .config(\"spark.ui.showConsoleProgress\", \"false\")\n",
    "            .getOrCreate()\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e1d9d3",
   "metadata": {},
   "source": [
    "### Define and prepare the input datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87bcf4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hfeatures: vector (nullable = true)\n",
      " |-- label: long (nullable = true)\n",
      " |-- lfeatures: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |-- hfeatures_dense: vector (nullable = true)\n",
      " |-- encoded_label: vector (nullable = true)\n",
      " |-- HLF_input: vector (nullable = true)\n",
      " |-- GRU_input: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      "\n",
      "root\n",
      " |-- hfeatures: vector (nullable = true)\n",
      " |-- label: long (nullable = true)\n",
      " |-- lfeatures: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |-- hfeatures_dense: vector (nullable = true)\n",
      " |-- encoded_label: vector (nullable = true)\n",
      " |-- HLF_input: vector (nullable = true)\n",
      " |-- GRU_input: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the datasets from https://github.com/cerndb/SparkDLTrigger/tree/master/Data\n",
    "#\n",
    "# For CERN users, data is available on EOS and HDFS\n",
    "# PATH = \"/eos/project/s/sparkdltrigger/public/\"\n",
    "PATH = \"hdfs://analytix/Training/Spark/TopologyClassifier/\"\n",
    "\n",
    "df_test_raw = spark.read.parquet(PATH + \"testUndersampled.parquet\")\n",
    "df_test_raw.printSchema()\n",
    "\n",
    "df_train_raw = spark.read.parquet(PATH + \"trainUndersampled.parquet\")\n",
    "df_train_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93f9e964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- HLF_input: array (nullable = false)\n",
      " |    |-- element: double (containsNull = false)\n",
      " |-- GRU_input: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- encoded_label: array (nullable = false)\n",
      " |    |-- element: double (containsNull = false)\n",
      "\n",
      "root\n",
      " |-- HLF_input: array (nullable = false)\n",
      " |    |-- element: double (containsNull = false)\n",
      " |-- GRU_input: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- encoded_label: array (nullable = false)\n",
      " |    |-- element: double (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extacts the columns of interest for the high level features classifier\n",
    "# Transform Vectors in Arrays\n",
    "# This is because we need Array for the Petastorm example notebook\n",
    "# Select the fields used by the Inclusive classifier: HLF_input, GRU_input and encoded_label\n",
    "# Note: GRU_input is flattened, this allows to use the Example record format (default),\n",
    "# restoring the Array to its original shape will be handled in TensorFlow using tf.data and tf.io\n",
    "\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.sql.functions import flatten\n",
    "\n",
    "df_test = ( df_test_raw\n",
    "             .withColumn('HLF_input', vector_to_array('HLF_input'))\n",
    "             .withColumn('GRU_input', flatten('GRU_input'))\n",
    "             .withColumn('encoded_label', vector_to_array('encoded_label'))\n",
    "             .select('HLF_input', 'GRU_input', 'encoded_label')\n",
    "          )\n",
    "\n",
    "df_test.printSchema()\n",
    "\n",
    "df_train = ( df_train_raw\n",
    "               .withColumn('HLF_input', vector_to_array('HLF_input'))\n",
    "               .withColumn('GRU_input', flatten('GRU_input'))\n",
    "               .withColumn('encoded_label', vector_to_array('encoded_label'))\n",
    "               .select('HLF_input', 'GRU_input', 'encoded_label')\n",
    "           )\n",
    "\n",
    "df_train.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f12e3aa",
   "metadata": {},
   "source": [
    "### Save the datasets in TFRecord format\n",
    "This will be used with TensorFLowto train the inclusive classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e481f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize the output path\n",
    "outputPATH = PATH\n",
    "#outputPATH = '/user/canali/training/'\n",
    "\n",
    "numPartitions = 200\n",
    "\n",
    "# Save the test dataset\n",
    "# compact output in numPartitions files using coalesce()\n",
    "df_test.coalesce(numPartitions).write.format(\"tfrecords\").save(outputPATH + \"testUndersampled_InclusiveClassifier.tfrecord\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e82f80ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for the training dataset\n",
    "\n",
    "# Save the training dataset in TFRecord format\n",
    "# compact output in numPartitions files using coalesce()\n",
    "df_train.coalesce(numPartitions).write.format(\"tfrecords\").save(outputPATH + \"trainUndersampled_InclusiveClassifier.tfrecord\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfa355a",
   "metadata": {},
   "source": [
    "### Save the datasets in Parquet format\n",
    "This will be used with Petastorm to train the inclusive classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b3ddd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.coalesce(numPartitions).write.parquet(outputPATH + \"testUndersampled_InclusiveClassifier.parquet\")\n",
    "\n",
    "df_train.coalesce(numPartitions).write.parquet(outputPATH + \"trainUndersampled_InclusiveClassifier.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aadeabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d4baac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
