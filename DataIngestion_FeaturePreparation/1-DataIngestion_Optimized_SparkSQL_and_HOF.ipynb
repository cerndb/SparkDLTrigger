{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion and Filtering - Pipeline for the Topology Classifier with Apache Spark\n",
    "## Optimized Using Spark SQL and Higher Order Functions (HOF)\n",
    "\n",
    "**1. Data Ingestion** is the first stage of the pipeline. Here we will read the ROOT file from HDFS into a Spark dataframe using [Spark-ROOT](https://github.com/diana-hep/spark-root) reader and then we will create the Low Level Features (LLF) and High Level Features datasets.\n",
    "\n",
    "To run this notebook we used the following configuration:\n",
    "* *Software stack*: Spark 2.4.3\n",
    "* *Platform*: CentOS 7, Python 3.6\n",
    "* *Spark cluster*: Analytix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pyspark or use your favorite way to set Spark Home, here we use findspark\n",
    "import findspark\n",
    "findspark.init('/home/luca/Spark/spark-2.4.3-bin-hadoop2.7') #set path to SPARK_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure according to your environment\n",
    "pyspark_python = \"<path to python>/bin/python\"\n",
    "spark_root_jar=\"https://github.com/diana-hep/spark-root/blob/master/jars/spark-root_2.11-0.1.17.jar?raw=true\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"1-Data Ingestion Optimized with Spark SQL and HOF\") \\\n",
    "        .master(\"yarn\") \\\n",
    "        .config(\"spark.driver.memory\",\"8g\") \\\n",
    "        .config(\"spark.executor.memory\",\"14g\") \\\n",
    "        .config(\"spark.executor.cores\",\"8\") \\\n",
    "        .config(\"spark.executor.instances\",\"50\") \\\n",
    "        .config(\"spark.dynamicAllocation.enabled\",\"false\") \\\n",
    "        .config(\"spark.jars\",spark_root_jar) \\\n",
    "        .config(\"spark.jars.packages\",\"org.diana-hep:root4j:0.1.6\") \\\n",
    "        .config(\"spark.pyspark.python\",pyspark_python) \\\n",
    "        .config(\"spark.eventLog.enabled\",\"false\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://pcitdbgpu1.dyndns.cern.ch:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>1-Data Ingestion Optimized with Spark SQL and HOF</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f9e7e227208>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a file containing functions that we will use later\n",
    "spark.sparkContext.addPyFile(\"utilFunctions.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data from HDFS\n",
    "<br>\n",
    "As first step we will read the samples into a Spark dataframe using Spark-Root. We will select only a subset of columns present in the original files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"hdfs://analytix/Training/Spark/TopologyClassifier/lepFilter_rawData/\"\n",
    "\n",
    "samples = [\"qcd_lepFilter_13TeV\", \"ttbar_lepFilter_13TeV\", \"Wlnu_lepFilter_13TeV\"]\n",
    "\n",
    "requiredColumns = [\n",
    "    \"EFlowTrack\",\n",
    "    \"EFlowNeutralHadron\",\n",
    "    \"EFlowPhoton\",\n",
    "    \"Electron\",\n",
    "    \"MuonTight\",\n",
    "    \"MuonTight_size\",\n",
    "    \"Electron_size\",\n",
    "    \"MissingET\",\n",
    "    \"Jet\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Wlnu_lepFilter_13TeV sample...\n",
      "Loading qcd_lepFilter_13TeV sample...\n",
      "Loading ttbar_lepFilter_13TeV sample...\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "dfList = []\n",
    "\n",
    "for label,sample in enumerate(samples):\n",
    "    print(\"Loading {} sample...\".format(sample))\n",
    "    tmpDF = spark.read \\\n",
    "                .format(\"org.dianahep.sparkroot.experimental\") \\\n",
    "                .load(PATH + sample + \"/*.root\") \\\n",
    "                .select(requiredColumns) \\\n",
    "                .withColumn(\"label\", lit(label))\n",
    "    dfList.append(tmpDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all samples into a single dataframe\n",
    "df = dfList[0]\n",
    "for tmpDF in dfList[1:]:\n",
    "    df = df.union(tmpDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at how many events there are for each class. Keep in mind that the labels are mapped as follow\n",
    "* $0=\\text{QCD}$\n",
    "* $1=\\text{t}\\bar{\\text{t}}$\n",
    "* $2=\\text{W}+\\text{jets}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the schema of one of the required columns. This shows that the  \n",
    "**schema is complex and nested** (the full schema is even more complex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- EFlowTrack: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- fUniqueID: integer (nullable = true)\n",
      " |    |    |-- fBits: integer (nullable = true)\n",
      " |    |    |-- PID: integer (nullable = true)\n",
      " |    |    |-- Charge: integer (nullable = true)\n",
      " |    |    |-- PT: float (nullable = true)\n",
      " |    |    |-- Eta: float (nullable = true)\n",
      " |    |    |-- Phi: float (nullable = true)\n",
      " |    |    |-- EtaOuter: float (nullable = true)\n",
      " |    |    |-- PhiOuter: float (nullable = true)\n",
      " |    |    |-- X: float (nullable = true)\n",
      " |    |    |-- Y: float (nullable = true)\n",
      " |    |    |-- Z: float (nullable = true)\n",
      " |    |    |-- T: float (nullable = true)\n",
      " |    |    |-- XOuter: float (nullable = true)\n",
      " |    |    |-- YOuter: float (nullable = true)\n",
      " |    |    |-- ZOuter: float (nullable = true)\n",
      " |    |    |-- TOuter: float (nullable = true)\n",
      " |    |    |-- Dxy: float (nullable = true)\n",
      " |    |    |-- SDxy: float (nullable = true)\n",
      " |    |    |-- Xd: float (nullable = true)\n",
      " |    |    |-- Yd: float (nullable = true)\n",
      " |    |    |-- Zd: float (nullable = true)\n",
      " |    |    |-- EFlowTrack_Particle: struct (nullable = true)\n",
      " |    |    |    |-- TObject: struct (nullable = true)\n",
      " |    |    |    |    |-- fUniqueID: integer (nullable = true)\n",
      " |    |    |    |    |-- fBits: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"EFlowTrack\").printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering with Spark SQL and Higher Order Functions (HOF)\n",
    "\n",
    "Spark SQL/Dataframe API can be used to write some of the filters needed for this work.  \n",
    "This has the advantage of better performance, Spark SQL runs typically faster than UDF filters as it avoid data serialization back and forth to Python.\n",
    "Additionally, Spark SQL Higher Order Functions, introduced in Spark from version 2.4.0, allow to espress operations on nested data (arrays), which are very important for our ue case.    \n",
    "A few examples of how this work in our case:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Filter muons and electrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a small subset of the data to illustrate how Spark SQL and HOF work\n",
    "df_test = df.limit(100)\n",
    "df_test.cache()\n",
    "df_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = df_test.filter((df['MuonTight_size']!=0) | (df['Electron_size']!=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Filter tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = df_test.select('EFlowTrack').take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "306"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tracks[0].EFlowTrack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(fUniqueID=7544, fBits=50331664, PID=211, Charge=1, PT=0.3664204180240631, Eta=-3.9139456748962402, Phi=-2.277653455734253, EtaOuter=-3.894503355026245, PhiOuter=-2.4723126888275146, X=0.0, Y=0.0, Z=78.35020446777344, T=6.139511815606014e-11, XOuter=-95.81527709960938, YOuter=-75.79782104492188, ZOuter=-3000.0, TOuter=1.0339084255406306e-08, Dxy=0.0, SDxy=0.0, Xd=0.0, Yd=0.0, Zd=78.35020446777344, EFlowTrack_Particle=Row(TObject=Row(fUniqueID=0, fBits=65536)))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracks[0].EFlowTrack[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original Python code with filters:\n",
    "\n",
    "```Python\n",
    "def ChPtMapp(DR, event):\n",
    "    pTmap = []\n",
    "    for h in event.EFlowTrack:\n",
    "        if h.PT<= 0.5: continue\n",
    "        pTmap.append([h.Eta, h.Phi, h.PT])\n",
    "    return np.asarray(pTmap)\n",
    "```\n",
    "\n",
    "hence we can use HOF to filer all the particles in `EFlowTrack` with $p_T\\leq0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered.createOrReplaceTempView(\"test_events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------------+\n",
      "|size(EFlowTrack)|EFlowTrack_Filtered|\n",
      "+----------------+-------------------+\n",
      "|             306|                238|\n",
      "|             472|                372|\n",
      "|             341|                250|\n",
      "|             389|                295|\n",
      "|             162|                108|\n",
      "+----------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT cardinality(EFlowTrack),\n",
    "    cardinality(FILTER(EFlowTrack,\n",
    "        tracks -> tracks.PT > 0.5\n",
    "    )) EFlowTrack_Filtered\n",
    "FROM test_events\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same thing for the others, for example consider the photons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+\n",
      "|size(EFlowPhoton)|EFlowPhoton_Filtered|\n",
      "+-----------------+--------------------+\n",
      "|              377|                  75|\n",
      "|              540|                  82|\n",
      "|              429|                  54|\n",
      "|              583|                  77|\n",
      "|              228|                  29|\n",
      "+-----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT cardinality(EFlowPhoton),\n",
    "    cardinality(FILTER(EFlowPhoton,\n",
    "        photon -> photon.ET > 1\n",
    "    )) EFlowPhoton_Filtered\n",
    "FROM test_events\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and also neutral hadrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+---------------------------+\n",
      "|size(EFlowNeutralHadron)|EFlowNeutralHadron_Filtered|\n",
      "+------------------------+---------------------------+\n",
      "|                     311|                         92|\n",
      "|                     426|                        136|\n",
      "|                     335|                         95|\n",
      "|                     422|                        138|\n",
      "|                     214|                         52|\n",
      "+------------------------+---------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT cardinality(EFlowNeutralHadron),\n",
    "    cardinality(FILTER(EFlowNeutralHadron,\n",
    "        hadron -> hadron.ET > 1\n",
    "    )) EFlowNeutralHadron_Filtered\n",
    "FROM test_events\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the simulation of the trigger we require $p_T > 23\\,\\text{GeV}$ for muons and electrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+-----------------+-------------+\n",
      "|size(Electron)|size(MuonTight)|Electron_Filtered|Muon_Filtered|\n",
      "+--------------+---------------+-----------------+-------------+\n",
      "|             0|              1|                0|            1|\n",
      "|             0|              1|                0|            1|\n",
      "|             0|              1|                0|            1|\n",
      "|             1|              0|                0|            0|\n",
      "|             0|              1|                0|            1|\n",
      "+--------------+---------------+-----------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    cardinality(Electron),\n",
    "    cardinality(MuonTight),\n",
    "    cardinality(FILTER(Electron,\n",
    "        electron -> electron.PT > 23\n",
    "    )) Electron_Filtered,\n",
    "    cardinality(FILTER(MuonTight,\n",
    "        muon -> muon.PT > 23\n",
    "    )) Muon_Filtered\n",
    "FROM test_events\n",
    "WHERE MuonTight_size > 0 OR Electron_size > 0\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-filtering with Spark SQL and High Level Function optimizations \n",
    "From here we do the actual processing of filtering data for our use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|label|   count|\n",
      "+-----+--------+\n",
      "|    2|26335315|\n",
      "|    1|13780026|\n",
      "|    0|14354796|\n",
      "| null|54470137|\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the number of events per sample and the total (label=null)\n",
    "df.rollup(\"label\").count().orderBy(\"label\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register the dataframe with events data as a temporary view\n",
    "df.createOrReplaceTempView(\"events\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter leptons (electrons and muons in this case)\n",
    "- require $p_T$ > 23 GEv for electron and muon\n",
    "- take events with at leat one electron or muon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filter leptons\n",
    "filteredLeptons = spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM (\n",
    "    SELECT\n",
    "        label,\n",
    "        EFlowTrack,\n",
    "        EFlowNeutralHadron,\n",
    "        EFlowPhoton ,\n",
    "        MissingET,\n",
    "        Jet,\n",
    "        FILTER(Electron, \n",
    "            electron -> electron.PT > 23\n",
    "        ) Electron,\n",
    "        FILTER(MuonTight,\n",
    "            muon -> muon.PT > 23\n",
    "        ) MuonTight\n",
    "    FROM events\n",
    "    WHERE MuonTight_size > 0 OR Electron_size > 0\n",
    ") Filtered \n",
    "WHERE cardinality(Electron) > 0 \n",
    "      OR cardinality(MuonTight) > 0\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredLeptons.createOrReplaceTempView(\"filteredLeptons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Decrease number of tracks\n",
    "filteredDF = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    label,\n",
    "    FILTER(EFlowTrack,\n",
    "        tracks -> tracks.PT > 0.5\n",
    "    ) EFlowTrack, \n",
    "    \n",
    "    FILTER(EFlowNeutralHadron,\n",
    "        hadron -> hadron.ET > 1.0\n",
    "    ) EFlowNeutralHadron,\n",
    "    \n",
    "    FILTER(EFlowPhoton,\n",
    "        photon -> photon.ET > 1.0\n",
    "    ) EFlowPhoton,\n",
    "    \n",
    "    FILTER(Jet,\n",
    "        jet -> ((jet.PT>30.0) AND (ABS(jet.Eta)<2.6)) \n",
    "    ) Jets,\n",
    "    \n",
    "    MissingET,\n",
    "    Electron,\n",
    "    MuonTight\n",
    "FROM filteredLeptons\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredDF.createOrReplaceTempView(\"filteredDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reduce number of features\n",
    "reduced = spark.sql(\"\"\"\n",
    "SELECT\n",
    "     label,\n",
    "     TRANSFORM(EFlowTrack,\n",
    "     track -> map_from_arrays(\n",
    "        Array(\"PT\", \"Eta\", \"Phi\", \"PID\", \"X\", \"Y\", \"Z\"),\n",
    "        Array(track.PT, track.Eta, track.Phi, track.PID, track.X, track.Y, track.Z)\n",
    "        )\n",
    "     ) Tracks,\n",
    "     \n",
    "     TRANSFORM(EFlowPhoton,\n",
    "     photon -> map_from_arrays(\n",
    "        Array(\"ET\", \"Eta\", \"Phi\"),\n",
    "        Array(photon.ET, photon.Eta, photon.Phi)\n",
    "        )\n",
    "     ) Photons,\n",
    "     \n",
    "     TRANSFORM(EFlowNeutralHadron,\n",
    "     hadron -> map_from_arrays(\n",
    "        Array(\"ET\", \"Eta\", \"Phi\"),\n",
    "        Array(hadron.ET, hadron.Eta, hadron.Phi)\n",
    "        )\n",
    "     ) NeutralHadrons,\n",
    "     \n",
    "     TRANSFORM(MissingET,\n",
    "     missingET -> map_from_arrays(\n",
    "        Array(\"MET\", \"Phi\"),\n",
    "        Array(missingET.MET, missingET.Phi)\n",
    "        )\n",
    "     ) MissingET,\n",
    "     \n",
    "     TRANSFORM(Jets,\n",
    "     jet -> map_from_arrays(\n",
    "        Array(\"PT\", \"BTag\"),\n",
    "        Array(jet.PT, jet.BTag)\n",
    "        )\n",
    "     ) Jets,\n",
    "     \n",
    "     TRANSFORM(Electron,\n",
    "     electron -> map_from_arrays(\n",
    "        Array(\"PT\", \"Eta\", \"Phi\", \"Charge\"),\n",
    "        Array(electron.PT, electron.Eta, electron.Phi, electron.Charge)\n",
    "        )\n",
    "     ) Electron,\n",
    "     \n",
    "     TRANSFORM(MuonTight,\n",
    "     muon -> map_from_arrays(\n",
    "        Array(\"PT\", \"Eta\", \"Phi\", \"Charge\"),\n",
    "        Array(muon.PT, muon.Eta, muon.Phi,muon.Charge)\n",
    "        )\n",
    "     ) MuonTight\n",
    "     \n",
    "FROM filteredDF\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced.createOrReplaceTempView(\"reduced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: integer (nullable = false)\n",
      " |-- Tracks: array (nullable = true)\n",
      " |    |-- element: map (containsNull = false)\n",
      " |    |    |-- key: string\n",
      " |    |    |-- value: float (valueContainsNull = true)\n",
      " |-- Photons: array (nullable = true)\n",
      " |    |-- element: map (containsNull = false)\n",
      " |    |    |-- key: string\n",
      " |    |    |-- value: float (valueContainsNull = true)\n",
      " |-- NeutralHadrons: array (nullable = true)\n",
      " |    |-- element: map (containsNull = false)\n",
      " |    |    |-- key: string\n",
      " |    |    |-- value: float (valueContainsNull = true)\n",
      " |-- MissingET: array (nullable = true)\n",
      " |    |-- element: map (containsNull = false)\n",
      " |    |    |-- key: string\n",
      " |    |    |-- value: float (valueContainsNull = true)\n",
      " |-- Jets: array (nullable = true)\n",
      " |    |-- element: map (containsNull = false)\n",
      " |    |    |-- key: string\n",
      " |    |    |-- value: float (valueContainsNull = true)\n",
      " |-- Electron: array (nullable = true)\n",
      " |    |-- element: map (containsNull = false)\n",
      " |    |    |-- key: string\n",
      " |    |    |-- value: float (valueContainsNull = true)\n",
      " |-- MuonTight: array (nullable = true)\n",
      " |    |-- element: map (containsNull = false)\n",
      " |    |    |-- key: string\n",
      " |    |    |-- value: float (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reduced.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of events: 54470137\n",
      "Number of events after filtering with Spark SQL and HOF: 37757705\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of events:\", df.count())\n",
    "print(\"Number of events after filtering with Spark SQL and HOF:\", reduced.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset\n",
      "+-----+--------+\n",
      "|label|count(1)|\n",
      "+-----+--------+\n",
      "|    0|14354796|\n",
      "|    1|13780026|\n",
      "|    2|26335315|\n",
      "| null|54470137|\n",
      "+-----+--------+\n",
      "\n",
      "After filtering with Spark SQL and HOF\n",
      "+-----+--------+\n",
      "|label|count(1)|\n",
      "+-----+--------+\n",
      "|    0| 9983864|\n",
      "|    1| 9743725|\n",
      "|    2|18030116|\n",
      "| null|37757705|\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the number of events per sample and the total (label=null)\n",
    "print(\"Original dataset\")\n",
    "spark.sql(\"select label, count(*) from events group by rollup(label) order by label nulls last\").show()\n",
    "\n",
    "print(\"After filtering with Spark SQL and HOF\")\n",
    "spark.sql(\"select label, count(*) from reduced group by rollup(label) order by label nulls last\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create derivate datasets\n",
    "\n",
    "Now we will create the LLF and HLF datasets. This is done by the function `convert` below which takes as input an event (i.e. the list of particles present in that event) and do the following steps:\n",
    "0. Start from the reduced dataframe computed above, with events with at least one isolated electron/muon \n",
    "1. Create the list of 801 particles and the 19 low level features for each of them\n",
    "2. Compute the high level features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Lorentz Vector and other functions for pTmaps\n",
    "from utilFunctions import *\n",
    "\n",
    "def selection(Electron, MuonTight, TrkPtMap, NeuPtMap, PhotonPtMap):\n",
    "    \"\"\"\n",
    "    This function simulates the trigger selection.\n",
    "    Part of the selection is implemented in previous cells using Spark SQL\n",
    "    \"\"\"\n",
    "    \n",
    "    foundMuon = None \n",
    "    foundEle =  None \n",
    "\n",
    "    l = LorentzVector()\n",
    "    for ele in Electron:\n",
    "\n",
    "        l.SetPtEtaPhiM(ele[\"PT\"], ele[\"Eta\"], ele[\"Phi\"], 0.)\n",
    "        \n",
    "        pfisoCh = PFIso(l, 0.3, TrkPtMap, True)\n",
    "        pfisoNeu = PFIso(l, 0.3, NeuPtMap, False)\n",
    "        pfisoGamma = PFIso(l, 0.3, PhotonPtMap, False)\n",
    "        if foundEle == None and (pfisoCh+pfisoNeu+pfisoGamma)<0.45:\n",
    "            foundEle = [l.E(), l.Px(), l.Py(), l.Pz(), l.Pt(), l.Eta(), l.Phi(),\n",
    "                        0., 0., 0., pfisoCh, pfisoGamma, pfisoNeu,\n",
    "                        0., 0., 0., 1., 0., float(ele[\"Charge\"])]\n",
    "    for muon in MuonTight:\n",
    "        #\n",
    "        # Has to replace the TLorentzVector functionality\n",
    "        #\n",
    "        l.SetPtEtaPhiM(muon[\"PT\"], muon[\"Eta\"], muon[\"Phi\"], 0.)\n",
    "        \n",
    "        pfisoCh = PFIso(l, 0.3, TrkPtMap, True)\n",
    "        pfisoNeu = PFIso(l, 0.3, NeuPtMap, False)\n",
    "        pfisoGamma = PFIso(l, 0.3, PhotonPtMap, False)\n",
    "        if foundMuon == None and (pfisoCh+pfisoNeu+pfisoGamma)<0.45:\n",
    "            foundMuon = [l.E(), l.Px(), l.Py(), l.Pz(), l.Pt(), l.Eta(), l.Phi(),\n",
    "                         0., 0., 0., pfisoCh, pfisoGamma, pfisoNeu,\n",
    "                         0., 0., 0., 0., 1., float(muon[\"Charge\"])]\n",
    "    if foundEle != None and foundMuon != None:\n",
    "        if foundEle[5] > foundMuon[5]:\n",
    "            return True, foundEle, foundMuon\n",
    "        else:\n",
    "            return True, foundMuon, foundEle\n",
    "    if foundEle != None: return True, foundEle, foundMuon\n",
    "    if foundMuon != None: return True, foundMuon, foundEle\n",
    "    return False, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import pandas_udf, udf, col\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "import numpy as np\n",
    "\n",
    "def convert(event):\n",
    "    \"\"\"\n",
    "    This function takes as input an event, applies trigger selection \n",
    "    and create LLF and HLF datasets\n",
    "    \"\"\"\n",
    "    q = LorentzVector()\n",
    "    particles = np.zeros((801,19))\n",
    "    index = 0\n",
    "    \n",
    "    TrkPtMap = ChPtMapp2(event.Tracks)\n",
    "    NeuPtMap = NeuPtMapp2(event.NeutralHadrons)\n",
    "    PhotonPtMap = PhotonPtMapp2(event.Photons)\n",
    "    \n",
    "    ## Lepton Filter\n",
    "    selected, lep, otherlep = selection(event.Electron, event.MuonTight,\n",
    "                                        TrkPtMap, NeuPtMap, PhotonPtMap)\n",
    "    if not selected: return Row()\n",
    "    #particles.append(lep)\n",
    "    particles[index] = lep\n",
    "    index\n",
    "    lepMomentum = LorentzVector(lep[1], lep[2], lep[3], lep[0])\n",
    "    \n",
    "    nTrk = 0\n",
    "    for h in event.Tracks:\n",
    "        if nTrk>=450: break\n",
    "        q.SetPtEtaPhiM(h[\"PT\"], h[\"Eta\"], h[\"Phi\"], 0.)\n",
    "        if lepMomentum.DeltaR(q) > 0.0001:\n",
    "            pfisoCh = PFIso(q, 0.3, TrkPtMap, True)\n",
    "            pfisoNeu = PFIso(q, 0.3, NeuPtMap, False)\n",
    "            pfisoGamma = PFIso(q, 0.3, PhotonPtMap, False)\n",
    "            \"\"\"particles.append([q.E(), q.Px(), q.Py(), q.Pz(),\n",
    "                              h[\"PT\"], h[\"Eta\"], h[\"Phi\"], h[\"X\"], h[\"Y\"], h[\"Z\"],\n",
    "                              pfisoCh, pfisoGamma, pfisoNeu,\n",
    "                              1., 0., 0., 0., 0., np.sign(h[\"PID\"])])\"\"\"\n",
    "            particles[index] = [q.E(), q.Px(), q.Py(), q.Pz(),\n",
    "                              h[\"PT\"], h[\"Eta\"], h[\"Phi\"], h[\"X\"], h[\"Y\"], h[\"Z\"],\n",
    "                              pfisoCh, pfisoGamma, pfisoNeu,\n",
    "                              1., 0., 0., 0., 0., np.sign(h[\"PID\"])]\n",
    "            nTrk += 1\n",
    "            index += 1\n",
    "            \n",
    "    nPhoton = 0\n",
    "    for h in event.Photons:\n",
    "        if nPhoton >= 150: break\n",
    "        q.SetPtEtaPhiM(h[\"ET\"], h[\"Eta\"], h[\"Phi\"], 0.)\n",
    "        pfisoCh = PFIso(q, 0.3, TrkPtMap, True)\n",
    "        pfisoNeu = PFIso(q, 0.3, NeuPtMap, False)\n",
    "        pfisoGamma = PFIso(q, 0.3, PhotonPtMap, False)\n",
    "        \"\"\"particles.append([q.E(), q.Px(), q.Py(), q.Pz(),\n",
    "                          h[\"ET\"], h[\"Eta\"], h[\"Phi\"], 0., 0., 0.,\n",
    "                          pfisoCh, pfisoGamma, pfisoNeu,\n",
    "                          0., 0., 1., 0., 0., 0.])\"\"\"\n",
    "        particles[index] = [q.E(), q.Px(), q.Py(), q.Pz(),\n",
    "                          h[\"ET\"], h[\"Eta\"], h[\"Phi\"], 0., 0., 0.,\n",
    "                          pfisoCh, pfisoGamma, pfisoNeu,\n",
    "                          0., 0., 1., 0., 0., 0.]\n",
    "        nPhoton += 1\n",
    "        index += 1\n",
    "    \n",
    "    nNeu = 0\n",
    "    for h in event.NeutralHadrons:\n",
    "        if nNeu >= 200: break\n",
    "        q.SetPtEtaPhiM(h[\"ET\"], h[\"Eta\"], h[\"Phi\"], 0.)\n",
    "        pfisoCh = PFIso(q, 0.3, TrkPtMap, True)\n",
    "        pfisoNeu = PFIso(q, 0.3, NeuPtMap, False)\n",
    "        pfisoGamma = PFIso(q, 0.3, PhotonPtMap, False)\n",
    "        \"\"\"particles.append([q.E(), q.Px(), q.Py(), q.Pz(),\n",
    "                          h[\"ET\"], h[\"Eta\"], h[\"Phi\"], 0., 0., 0.,\n",
    "                          pfisoCh, pfisoGamma, pfisoNeu,\n",
    "                          0., 1., 0., 0., 0., 0.])\"\"\"\n",
    "        particles[index] = [q.E(), q.Px(), q.Py(), q.Pz(),\n",
    "                          h[\"ET\"], h[\"Eta\"], h[\"Phi\"], 0., 0., 0.,\n",
    "                          pfisoCh, pfisoGamma, pfisoNeu,\n",
    "                          0., 1., 0., 0., 0., 0.]\n",
    "        nNeu += 1\n",
    "        index += 1\n",
    "    #    \n",
    "    # Build high level features\n",
    "    #\n",
    "    myMET = event.MissingET[0]\n",
    "    MET = myMET[\"MET\"]\n",
    "    phiMET = myMET[\"Phi\"]\n",
    "    MT = 2.*MET*lepMomentum.Pt()*(1-math.cos(lepMomentum.Phi()-phiMET))\n",
    "    HT = 0.\n",
    "    nJets = 0.\n",
    "    nBjets = 0.\n",
    "    for jet in event.Jets:\n",
    "        nJets += 1\n",
    "        HT += jet[\"PT\"]\n",
    "        if jet[\"BTag\"]>0: \n",
    "            nBjets += 1\n",
    "    LepPt = lep[4]\n",
    "    LepEta = lep[5]\n",
    "    LepPhi = lep[6]\n",
    "    LepIsoCh = lep[10]\n",
    "    LepIsoGamma = lep[11]\n",
    "    LepIsoNeu = lep[12]\n",
    "    LepCharge = lep[18]\n",
    "    LepIsEle = lep[16]\n",
    "    \n",
    "    hlf = [HT, MET, phiMET, MT, nJets, nBjets, LepPt, LepEta, LepPhi,\n",
    "           LepIsoCh, LepIsoGamma, LepIsoNeu, LepCharge, LepIsEle]\n",
    "    #\n",
    "    # return the Row of low level features and high level features\n",
    "    #\n",
    "    return Row(lfeatures=particles.tolist(), hfeatures=hlf, label=event.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally apply the function to all the events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = reduced.rdd \\\n",
    "            .map(convert) \\\n",
    "            .filter(lambda row: len(row) > 0) \\\n",
    "            .toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hfeatures: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- label: long (nullable = true)\n",
      " |-- lfeatures: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the datasets as Parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 812 ms, sys: 548 ms, total: 1.36 s\n",
      "Wall time: 2h 11min 23s\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"hdfs://analytix/Training/Spark/TopologyClassifier/dataIngestion_full_13TeV\"\n",
    "num_partitions = 3000 # used in DataFrame coalesce operation to limit number of output files\n",
    "\n",
    "%time features.coalesce(num_partitions).write.partitionBy(\"label\").parquet(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of events written to Parquet: 25468476\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of events written to Parquet:\", spark.read.parquet(dataset_path).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
