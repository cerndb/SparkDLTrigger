{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traininig the Inclusive classifier with tf.Keras using data in TFRecord format\n",
    "\n",
    "**tf.keras Inclusive classifier** This notebooks trains a neural network for the particle classifier using the Inclusive Classifier, using as input the list of recunstructed particles with the low level features + the high level features. Data is prepared from Parquet using Apache Spark, and written into TFRecord format. Data in TFRecord format is read from TensorFlow using tf.data and tf.io in tf.keras.\n",
    "\n",
    "To run this notebook we used the following configuration:\n",
    "* *Software stack*: TensorFlow 2.0.1\n",
    "* *Platform*: CentOS 7, Python 3.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Sequential, Input, Model\n",
    "from tensorflow.keras.layers import Masking, Dense, Activation, GRU, Dropout, concatenate\n",
    "\n",
    "tf.version.VERSION\n",
    "# only needed for TensorFlow 1.x\n",
    "# tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure distributed training using tf.distribute\n",
    "This notebook shows an example of distributed training with tf.keras using 4 concurrent executions on a single machine.\n",
    "The test machine has 24 physical cores it has been notes that a serial execution of the training would leave spare capacity. With distributed training we can \"use all the CPU on the box\". \n",
    "- TensorFlow MultiWorkerMirroredStrategy is used to distribute the training.\n",
    "- Configuration of the workers is done using the OS enviroment variable **TF_CONFIG**.\n",
    "- **nodes_endpoints** configures the list of machines and ports that will be used. In this example, we use 3 workers on the same machines, you can use this to distribute over multiple machines too\n",
    "- **worker_number** will be unique for each worker, numbering starts from 0\n",
    "- Worker number 0 will be the master. \n",
    "- You need to run the 4 notebooks for the 4 configured workers at the same time (training will only start when all 4 workers are active) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each worker will have a unique worker_number, numbering starts from 0\n",
    "worker_number=2\n",
    "\n",
    "nodes_endpoints = [\"localhost:12345\", \"localhost:12346\", \"localhost:12347\", \"localhost:12348\"]\n",
    "number_workers = len(nodes_endpoints)\n",
    "\n",
    "import os\n",
    "import json\n",
    "os.environ['TF_CONFIG'] = json.dumps({\n",
    "    'cluster': {\n",
    "        'worker': nodes_endpoints\n",
    "    },\n",
    "    'task': {'type': 'worker', 'index': worker_number}\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Keras model for the inclusive classifier hooking with tf.distribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This implements the distributed stratedy for model\n",
    "with strategy.scope():\n",
    "    ## GRU branch\n",
    "    gru_input = Input(shape=(801,19), name='gru_input')\n",
    "    a = gru_input\n",
    "    a = Masking(mask_value=0.)(a)\n",
    "    a = GRU(units=50,activation='tanh')(a)\n",
    "    gruBranch = Dropout(0.2)(a)\n",
    "    \n",
    "    hlf_input = Input(shape=(14), name='hlf_input')\n",
    "    b = hlf_input\n",
    "    hlfBranch = Dropout(0.2)(b)\n",
    "\n",
    "    c = concatenate([gruBranch, hlfBranch])\n",
    "    c = Dense(25, activation='relu')(c)\n",
    "    output = Dense(3, activation='softmax')(c)\n",
    "    \n",
    "    model = Model(inputs=[gru_input, hlf_input], outputs=output)\n",
    "    \n",
    "    ## Compile model\n",
    "    optimizer = 'Adam'\n",
    "    loss = 'categorical_crossentropy'\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=[\"accuracy\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "gru_input (InputLayer)          [(None, 801, 19)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking (Masking)               (None, 801, 19)      0           gru_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gru (GRU)                       (None, 50)           10650       masking[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "hlf_input (InputLayer)          [(None, 14)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 50)           0           gru[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 14)           0           hlf_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 64)           0           dropout[0][0]                    \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 25)           1625        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 3)            78          dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 12,353\n",
      "Trainable params: 12,353\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test and training data in TFRecord format, using tf.data and tf.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/local3/lucatests/Data/\"\n",
    "\n",
    "# test dataset \n",
    "files_test_dataset = tf.data.Dataset.list_files(PATH + \"testUndersampled.tfrecord/part-r*\", shuffle=False)\n",
    "\n",
    "# training dataset \n",
    "files_train_dataset = tf.data.Dataset.list_files(PATH + \"trainUndersampled.tfrecord/part-r*\", seed=4242)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tunable\n",
    "num_parallel_reads=16\n",
    "\n",
    "test_dataset = files_test_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE).interleave(\n",
    "    tf.data.TFRecordDataset, \n",
    "    cycle_length=num_parallel_reads,\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "train_dataset = files_train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE).interleave(\n",
    "    tf.data.TFRecordDataset, cycle_length=num_parallel_reads,\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to decode TF records into the required features and labels\n",
    "def decode(serialized_example):\n",
    "    deser_features = tf.io.parse_single_example(\n",
    "      serialized_example,\n",
    "      # Defaults are not specified since both keys are required.\n",
    "      features={\n",
    "          'HLF_input': tf.io.FixedLenFeature((14), tf.float32),\n",
    "          'GRU_input': tf.io.FixedLenFeature((801,19), tf.float32),\n",
    "          'encoded_label': tf.io.FixedLenFeature((3), tf.float32),\n",
    "          })\n",
    "    return((deser_features['GRU_input'], deser_features['HLF_input']), deser_features['encoded_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use for debugging\n",
    "# for record in test_dataset.take(1):\n",
    "#     print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_test_dataset=test_dataset.map(decode, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "parsed_train_dataset=train_dataset.map(decode, num_parallel_calls=tf.data.experimental.AUTOTUNE).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use for debugging\n",
    "# Show and example of the parsed data\n",
    "# for record in parsed_test_dataset.take(1):\n",
    "#    print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: (((None, 801, 19), (None, 14)), (None, 3)), types: ((tf.float32, tf.float32), tf.float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tunable\n",
    "batch_size = 128 * number_workers\n",
    "\n",
    "train=parsed_train_dataset.batch(batch_size)\n",
    "train=train.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "train=train.repeat()\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6691"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_train_samples=3426083   # there are 3426083 samples in the training dataset\n",
    "\n",
    "steps_per_epoch=num_train_samples//batch_size\n",
    "steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tunable\n",
    "test_batch_size = 1024 \n",
    "\n",
    "test=parsed_test_dataset.batch(test_batch_size)\n",
    "test=test.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "test=test.repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "836"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_test_samples=856090 # there are 856090 samples in the test dataset\n",
    "\n",
    "validation_steps=num_test_samples//test_batch_size  \n",
    "validation_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the tf.keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0311 13:38:07.836854 139635596392256 distribute_coordinator.py:825] `eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\n",
      "W0311 13:38:07.838634 139635596392256 distribute_coordinator.py:829] `eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\n",
      "W0311 13:38:07.840763 139635596392256 distributed_training_utils.py:1163] ModelCheckpoint callback is not provided. Workers will need to restart training if any fails.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 6691 steps\n",
      "Epoch 1/6\n",
      "6691/6691 [==============================] - 4680s 699ms/step - loss: 0.2853 - accuracy: 0.8928\n",
      "Epoch 2/6\n",
      "6691/6691 [==============================] - 4657s 696ms/step - loss: 0.1951 - accuracy: 0.9284\n",
      "Epoch 3/6\n",
      "6691/6691 [==============================] - 4641s 694ms/step - loss: 0.1697 - accuracy: 0.9382\n",
      "Epoch 4/6\n",
      "6691/6691 [==============================] - 4641s 694ms/step - loss: 0.1557 - accuracy: 0.9435\n",
      "Epoch 5/6\n",
      "6691/6691 [==============================] - 4632s 692ms/step - loss: 0.1481 - accuracy: 0.9465\n",
      "Epoch 6/6\n",
      "6691/6691 [==============================] - 4622s 691ms/step - loss: 0.1421 - accuracy: 0.9487\n",
      "CPU times: user 1d 1h 50min 28s, sys: 6h 24min 24s, total: 1d 8h 14min 53s\n",
      "Wall time: 7h 44min 36s\n"
     ]
    }
   ],
   "source": [
    "# train the Keras model\n",
    "\n",
    "# tunable\n",
    "num_epochs = 6\n",
    "\n",
    "# callbacks = [ tf.keras.callbacks.TensorBoard(log_dir='./logs') ]\n",
    "callbacks = []\n",
    "    \n",
    "%time history = model.fit(train, steps_per_epoch=steps_per_epoch, \\\n",
    "                          epochs=num_epochs, callbacks=callbacks, verbose=1)\n",
    "\n",
    "                          #validation_data=test, validation_steps=validation_steps, \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=\"/local1/lucatests/SparkDLTrigger/Training_TFKeras_Distributed/model/\"\n",
    "model.save(PATH + \"mymodel\" + str(worker_number) + \".h5\", save_format='h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training history performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt \n",
    "plt.style.use('seaborn-darkgrid')\n",
    "# Graph with loss vs. epoch\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(\"HLF classifier loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph with accuracy vs. epoch\n",
    "%matplotlib notebook\n",
    "plt.figure()\n",
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='validation')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title(\"HLF classifier accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model performance metrics\n",
    "Load the model and plot the ROC and AUC and te confusion matrix using the noteboook  \n",
    "**4.3a-Model_evaluate_ROC_and_CM.ipynb**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "sparkconnect": {
   "bundled_options": [],
   "list_of_options": [
    {
     "name": "spark.dynamicAllocation.enabled",
     "value": "false"
    },
    {
     "name": "spark.executor.memory",
     "value": "14G"
    },
    {
     "name": "spark.executor.cores",
     "value": "6"
    },
    {
     "name": "spark.executor.instances",
     "value": "6"
    },
    {
     "name": "spark.driver.memory",
     "value": "10G"
    },
    {
     "name": "spark.driver.maxResultSize",
     "value": "10G"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
