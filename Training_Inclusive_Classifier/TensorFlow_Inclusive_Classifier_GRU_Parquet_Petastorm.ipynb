{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traininig the Inclusive classifier with tf.keras using data in Parquet format with Petastorm\n",
    "\n",
    "**tf.keras Inclusive classifier, GRU-based model** This notebooks trains a neural network for the particle classifier using the Inclusive Classifier, using as input the full list of recunstructed particles and the High Level Features. Data is prepared in Parquet and ingested via Petastorm. Tensorflow data processing uses tf.data.  \n",
    "\n",
    "Credits: this notebook is part of the work: \n",
    "- [Machine Learning Pipelines with Modern Big Data Tools for High Energy Physics Comput Softw Big Sci 4, 8 (2020)](https://rdcu.be/b4Wk9)  \n",
    "- Code and data at:https://github.com/cerndb/SparkDLTrigger  \n",
    "\n",
    "The model is a classifier implemented as the concatenation of a Dense Neural Network and a Recurrent Neural Network (GRU)\n",
    " - input: 14 high-level features and an array of 801 particles with 19 low-level features, described in [ Topology classification with deep learning to improve real-time event selection at the LHC](https://link.springer.com/epdf/10.1007/s41781-019-0028-1?author_access_token=eTrqfrCuFIP2vF4nDLnFfPe4RwlQNchNByi7wbcMAY7NPT1w8XxcX1ECT83E92HWx9dJzh9T9_y5Vfi9oc80ZXe7hp7PAj21GjdEF2hlNWXYAkFiNn--k5gFtNRj6avm0UukUt9M9hAH_j4UR7eR-g%3D%3D)\n",
    " - output: 3 classes, \"W + jet\", \"QCD\", \"t tbar\", see also [Machine Learning Pipelines with Modern Big Data Tools for High Energy Physics Comput Softw Big Sci 4, 8 (2020)](https://rdcu.be/b4Wk9)  \n",
    " - Open dataset: [download data](https://github.com/cerndb/SparkDLTrigger/tree/master/Data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Keras model for the inclusive classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Sequential, Input, Model\n",
    "from tensorflow.keras.layers import Masking, Dense, Activation, GRU, Dropout, concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that we have a GPU available\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LSTM branch\n",
    "gru_input = Input(shape=(801,19), name='gru_input')\n",
    "a = gru_input\n",
    "a = Masking(mask_value=0.)(a)\n",
    "a = GRU(units=50,activation='tanh')(a)\n",
    "gruBranch = Dropout(0.2)(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hlf_input = Input(shape=(14,), name='hlf_input')\n",
    "b = hlf_input\n",
    "hlfBranch = Dropout(0.2)(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = concatenate([gruBranch, hlfBranch])\n",
    "c = Dense(25, activation='relu')(c)\n",
    "output = Dense(3, activation='softmax')(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[gru_input, hlf_input], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compile model\n",
    "optimizer = 'Adam'\n",
    "loss = 'categorical_crossentropy'\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=[\"accuracy\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test and training data in Parquet format, using Petastorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the datasets from \n",
    "# ** https://github.com/cerndb/SparkDLTrigger/tree/master/Data **\n",
    "#\n",
    "# For CERN users, data is already available on EOS\n",
    "PATH = \"file:///eos/project/s/sparkdltrigger/public/\"\n",
    "\n",
    "file_train_dataset = PATH + \"trainUndersampled_InclusiveClassifier.parquet\"\n",
    "file_test_dataset = PATH + \"testUndersampled_InclusiveClassifier.parquet\"\n",
    "\n",
    "# PATH needs to be \n",
    "# \"file://<full_path>_on_filesystem/Parquet_folder/\"\n",
    "# \"hdfs://<full_path_on_hdfs>/Parquet_folder/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the petastorm libary to load and feed the training and test data in Parquet format\n",
    "# It makes use TensorFLow tf.data.dataset\n",
    "\n",
    "import petastorm\n",
    "from petastorm import make_batch_reader\n",
    "from petastorm.tf_utils import make_petastorm_dataset\n",
    "\n",
    "petastorm.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the tf.keras model feeding data with Petastorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Train with TensorFlow using Petastorm to read Parquet files\n",
    "# This performs a rebatching operation on the training dataset  to set explicitly the bach size,\n",
    "# as otherwise Petastorm produces batches with Parquet rowgroup size, which is often too large. \n",
    "# \n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "with make_batch_reader(file_test_dataset, num_epochs = 1, shuffle_row_groups = False) as test_data:\n",
    "    with make_batch_reader(file_train_dataset, num_epochs = 1, shuffle_row_groups = False) as train_data:\n",
    "        # print(\"Number of training rows:\", train_data.dataset.read().num_rows)\n",
    "        #\n",
    "        # Transform Parquet files into TensorFlow datasets (tf.data API)\n",
    "        #\n",
    "        test_dataset = ( make_petastorm_dataset(test_data)\n",
    "                            .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "                            .map(lambda x: ((tf.reshape(x.GRU_input, [-1, 801, 19]), x.HLF_input), x.encoded_label))\n",
    "                       )\n",
    "        # use for debug\n",
    "        # for record in test_dataset.take(1):\n",
    "        #     print(record)\n",
    "        train_dataset = ( make_petastorm_dataset(train_data)\n",
    "                            .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "                            .map(lambda x: ((tf.reshape(x.GRU_input, [-1, 801, 19]), x.HLF_input), x.encoded_label))\n",
    "                            .unbatch()  # change this for rebatch with tensorflow 2.11\n",
    "                            .batch(batch_size)\n",
    "                        )       \n",
    "        #\n",
    "        # Train the Keras model\n",
    "        #\n",
    "        num_epochs = 6\n",
    "        %time history = model.fit(train_dataset, validation_data = test_dataset, \\\n",
    "                                  epochs = num_epochs, verbose=1)                               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "# tf.keras.models.save_model(model, PATH+\"mymodel\" + \".tf\", save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt \n",
    "plt.style.use('seaborn-darkgrid')\n",
    "# Graph with loss vs. epoch\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(\"HLF classifier loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph with accuracy vs. epoch\n",
    "%matplotlib notebook\n",
    "plt.figure()\n",
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='validation')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title(\"HLF classifier accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.load_model(\"./mymodel.tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Need to use workers_count=1 to avoid getting data potentially in different order at each execution\n",
    "with make_batch_reader(file_test_dataset, num_epochs = 1, workers_count=1, shuffle_row_groups = False, shuffle_rows=False) as test_data:\n",
    "    y_pred = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Need to use workers_count=1 to avoid getting data potentially in different order at each execution\n",
    "with make_batch_reader(file_test_dataset, num_epochs = 1, workers_count=1, shuffle_row_groups = False, shuffle_rows=False) as test_data:\n",
    "    y_true = np.concatenate([labels for features,labels in test_data])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Accuracy of the classifier: {:.4f}'.format(\n",
    "    accuracy_score(np.argmax(y_true, axis=1),np.argmax(y_pred, axis=1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "labels_name = ['qcd', 'tt', 'wjets']\n",
    "labels = [0,1,2]\n",
    "\n",
    "cm = confusion_matrix(np.argmax(y_true, axis=1), np.argmax(y_pred, axis=1), labels=labels)\n",
    "\n",
    "## Normalize CM\n",
    "cm = cm / cm.astype(float).sum(axis=1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax = sns.heatmap(cm, annot=True, fmt='g')\n",
    "ax.xaxis.set_ticklabels(labels_name)\n",
    "ax.yaxis.set_ticklabels(labels_name)\n",
    "plt.xlabel('True labels')\n",
    "plt.ylabel('Predicted labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC and AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(3):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_true[:, i], y_pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dictionary containign ROC-AUC for the three classes \n",
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "# Plot roc curve \n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr[0], tpr[0], lw=2, \n",
    "         label='HLF classifier (AUC) = %0.4f' % roc_auc[0])\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Background Contamination (FPR)')\n",
    "plt.ylabel('Signal Efficiency (TPR)')\n",
    "plt.title('$tt$ selector')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "sparkconnect": {
   "bundled_options": [],
   "list_of_options": [
    {
     "name": "spark.dynamicAllocation.enabled",
     "value": "false"
    },
    {
     "name": "spark.executor.memory",
     "value": "14G"
    },
    {
     "name": "spark.executor.cores",
     "value": "6"
    },
    {
     "name": "spark.executor.instances",
     "value": "6"
    },
    {
     "name": "spark.driver.memory",
     "value": "10G"
    },
    {
     "name": "spark.driver.maxResultSize",
     "value": "10G"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
